from elasticsearch import Elasticsearch, helpers
from elasticsearch.helpers import bulk
from sqlalchemy import text
from backend.database_config import Session
from openai import OpenAI
import os
from dotenv import load_dotenv
from sentry_sdk import capture_exception
from backend.tables import internships_table

"""
internships_elasticsearch_config.py is a file that contains functions involving the indexes in elasticsearch. It handles 
index creation, document insertion, and anything to do with changing/updating the elasticsearch indexes.

Any new function or changes to the configuration of elasticsearch should be added here.
"""

load_dotenv()

# Password for the 'elastic' user generated by Elasticsearch
ELASTIC_PASSWORD = os.getenv("ELASTIC_PASSWORD")

# Found in the 'Manage Deployment' page
CLOUD_ID = os.getenv("CLOUD_ID")

# Create the client instance
elasticsearch_client = Elasticsearch(
    cloud_id=CLOUD_ID,
    basic_auth=("elastic", ELASTIC_PASSWORD)
)


# Define the Azure OpenAI credentials
ENDPOINT = os.getenv('OPENAI_API_ENDPOINT')
ADD_TITLE_API_KEY = os.getenv('ADD_TITLE_API_KEY')
REFRESH_JOBS_API_KEY = os.getenv('REFRESH_JOBS_API_KEY')
MAIN_SEARCH_API_KEY = os.getenv('MAIN_SEARCH_API_KEY')
ELASTICSEARCH_EMBEDDINGS_API_KEY = os.getenv('ELASTICSEARCH_EMBEDDINGS_API_KEY')
API_VERSION = "2024-02-01"
TITLE_MODEL_NAME = "text-embedding-3-small"  # Using small text embeddings for job titles
DESCRIPTION_MODEL_NAME = "text-embedding-3-small"  # Using small text embeddings for job descriptions

# Create the OPENAI client for use
add_title_client = OpenAI(
    api_key=ADD_TITLE_API_KEY,
    organization=os.getenv('OPENAI_ORGANIZATION'),
)

refresh_jobs_client = OpenAI(
    api_key=REFRESH_JOBS_API_KEY,
    organization=os.getenv('OPENAI_ORGANIZATION'),
)

main_search_client = OpenAI(
    api_key=MAIN_SEARCH_API_KEY,
    organization=os.getenv('OPENAI_ORGANIZATION'),
)

elasticsearch_embeddings_client = OpenAI(
    api_key=ELASTICSEARCH_EMBEDDINGS_API_KEY,
    organization=os.getenv('OPENAI_ORGANIZATION'),
)


def get_title_embeddings(jobs):
    """
    This function gets all the title embeddings for a list of jobs given

    Takes in a list of job dicts with 'id' and 'title' keys,
    generates embeddings for all the 'title's for those jobs,
    and returns a list of dicts with 'id', 'title', and 'title_embedding'.

    :return: a list of dicts with 'id', 'title', and 'title_embedding'.
    """
    try:
        # Extract descriptions and keep track of job IDs
        titles = [job['title'] for job in jobs]
        ids = [job['id'] for job in jobs]

        response = elasticsearch_embeddings_client.embeddings.create(
            input=titles,
            model=TITLE_MODEL_NAME,
            dimensions=256  # Create 256 dim embeddings for the titles
        )

        embeddings = [item.embedding for item in response.data]  # The embeddings created

        # Combine IDs with their corresponding embeddings and titles
        return [{'id': job_id, 'title': title, 'title_embedding': embedding} for job_id, title, embedding in
                zip(ids, titles, embeddings)]

    except Exception as e:
        capture_exception(e)
        return [{'id': job['id'], 'title': job['title'], 'title_embedding': None} for job in jobs]



def get_description_embeddings(jobs):
    """
        This function gets all the description embeddings for a list of jobs given

        Takes in a list of job dicts with 'id' and 'description' keys,
        generates embeddings for all the 'description's for those jobs,
        and returns a list of dicts with 'id' and 'description_embedding'.

        :return: a list of dicts with 'id' and 'description_embedding'.
        """
    try:
        # Extract descriptions and keep track of job IDs
        descriptions = [job['description'] for job in jobs]
        ids = [job['id'] for job in jobs]

        response = elasticsearch_embeddings_client.embeddings.create(
            input=descriptions,
            model=DESCRIPTION_MODEL_NAME,
            dimensions=256  # 256 dim embedding
        )

        embeddings = [item.embedding for item in response.data]  # List of the description embeddings

        # Combine IDs with their corresponding embeddings
        return [{'id': job_id, 'embedding': embedding} for job_id, embedding in zip(ids, embeddings)]

    except Exception as e:
        capture_exception(e)
        return [{'id': job['id'], 'embedding': None} for job in jobs]


def create_title_embeddings_internships_index():
    """
    Create the index for holding title embeddings for internships. It has the columns: job_id, title, and title_embedding

    :return: None
    """
    try:
    # Create the index with OpenAI embedding mappings
        resp = elasticsearch_client.indices.create(
            index="title-embeddings-internships",
            mappings={
                "properties": {
                    "title_embedding": {  # To hold embeddings for titles
                        "type": "dense_vector",
                        "dims": 256,  # 256 dimensions to match the title embeddings
                        "element_type": "float",  # Specify element type
                        "similarity": "dot_product"  # Use "dot_product" for semantic similarity
                    },
                    "title": {  # To hold the text of the actual job title
                        "type": "text"
                    },
                    "job_id": {
                        "type": "integer"
                    }
                }
            },
        )
        print(resp)
    except Exception as e:
        capture_exception(e)



def load_title_embeddings_internships_bulk():
    """
    This function loads the title embeddings into the index for any job that is in the internships table, but not yet
    in the title-embeddings-internships index. It first gets which jobs it needs to load, generates the embeddings, then loads them
    into the index.

    :return: None
    """
    session = Session
    existing_ids = set()

    try:

        # Step 1: Fetch existing job IDs from Elasticsearch
        scroll = elasticsearch_client.search(
            index="title-embeddings-internships",
            scroll="2m",
            body={"query": {"match_all": {}}, "_source": ["job_id"]},
            size=10000
        )

        while scroll["hits"]["hits"]:
            for hit in scroll["hits"]["hits"]:
                existing_ids.add(hit["_source"]["job_id"])
            scroll = elasticsearch_client.scroll(
                scroll_id=scroll["_scroll_id"],
                scroll="2m"
            )

        # Step 2: Fetch only non-indexed jobs from PostgreSQL
        if existing_ids:
            query = text(f'''
                    SELECT j.id AS job_id, j.title AS title
                    FROM {internships_table} j
                    WHERE j.id NOT IN :existing_ids
                ''')
            results = session.execute(query, {"existing_ids": tuple(existing_ids)}).fetchall()
        else:
            # If no existing IDs, just grab all jobs
            query = text(f'''
                    SELECT j.id AS job_id, j.title AS title
                    FROM {internships_table} j
                ''')
            results = session.execute(query).fetchall()

        # Just get the id and the title from the jobs, because this is all we need
        jobs = [{'id': row.job_id, 'title': row.title} for row in results]

        # Step 3: Process in batches and upload to Elasticsearch
        total_uploaded = 0
        while jobs:
            # Do in batches of 50
            jobs_batch = jobs[:50]
            jobs = jobs[50:]

            # Get embeddings
            embedded_jobs = get_title_embeddings(jobs_batch)

            # Prepare Elasticsearch bulk index actions
            actions = [
                {
                    "_index": "title-embeddings-internships",
                    "_source": {
                        "job_id": job["id"],
                        "title": job["title"],
                        "title_embedding": job["title_embedding"]
                    }
                }
                for job in embedded_jobs if job["title_embedding"] is not None
            ]

            # Bulk upload to Elasticsearch
            if actions:
                helpers.bulk(elasticsearch_client, actions)
                total_uploaded += len(actions)

    except Exception as e:
        capture_exception(e)


def load_title_embeddings_internships_bulk_from_jobs(jobs):
    """
        This function loads the title embeddings into the index for any job from the jobs given, but not yet
        in the title-embeddings-internships index. It first gets which jobs it needs to load, generates the embeddings, then loads
        them into the index

        :parameter jobs: A list of jobs to be used to load embeddings with
        :return: None
    """

    try:

        # Step 1: Get the job id and title from the jobs given
        jobs = [{'id': job['id'], 'title': job['job_title'] if 'job_title' in job else job['title']} for job in jobs]
        job_ids = [job['id'] for job in jobs]

        # Step 2: Check which IDs already exist in Elasticsearch
        existing_ids = set()
        query = {
            "query": {
                "terms": {
                    "job_id": job_ids
                }
            },
            "_source": ["job_id"]
        }

        try:
            response = elasticsearch_client.search(index="title-embeddings-internships", body=query, size=len(job_ids))
            existing_ids = {hit["_source"]["job_id"] for hit in response["hits"]["hits"]}
        except Exception as e:
            capture_exception(e)

        # Step 3: Filter out jobs that already exist
        jobs = [job for job in jobs if job["id"] not in existing_ids]

        total_uploaded = 0
        while jobs:
            # Load in batches of 50
            jobs_batch = jobs[:50]
            jobs = jobs[50:]

            # Get embeddings
            embedded_jobs = get_title_embeddings(jobs_batch)

            # Prepare Elasticsearch bulk index actions
            actions = [
                {
                    "_index": "title-embeddings-internships",
                    "_source": {
                        "job_id": job["id"],
                        "title": job["title"],
                        "title_embedding": job["title_embedding"]
                    }
                }
                for job in embedded_jobs if job["title_embedding"] is not None
            ]

            # Bulk upload to Elasticsearch
            if actions:
                helpers.bulk(elasticsearch_client, actions)
                total_uploaded += len(actions)

    except Exception as e:
        capture_exception(e)


def get_title_document_count_internships():
    """
    Retrieve the total count of documents in the Elasticsearch title-embeddings index.
    """
    try:
        total_docs = elasticsearch_client.count(index="title-embeddings-internships")["count"]
        return total_docs
    except Exception as e:
        capture_exception(e)
        return None


def create_description_embeddings_internships_index():
    """
        Create the index for holding description embeddings. It has the columns: job_id and description_embeddings

        :return: None
        """
    try:
        resp = elasticsearch_client.indices.create(
            index="description-embeddings-internships",
            mappings={
                "properties": {
                    "description_embedding": {  # To hold description embeddings
                        "type": "dense_vector",
                        "dims": 256,  # Match the 256 dim description embeddings
                        "element_type": "float",
                        "similarity": "dot_product"  # Use "dot_product" for semantic similarity
                    },
                    "job_id": {
                        "type": "integer"
                    }
                }
            },
        )
        print(resp)
    except Exception as e:
        capture_exception(e)


def load_description_embeddings_internships_bulk():
    """
        This function loads the description embeddings into the index for any job that is in the internships table, but not
        yet in the description-embeddings index. It first gets which jobs it needs to load, generates the embeddings,
        then loads them into the index.

        :return: None
    """
    session = Session
    existing_ids = set()

    try:

        # Step 1: Fetch existing job IDs from Elasticsearch description-embeddings index
        scroll = elasticsearch_client.search(
            index="description-embeddings-internships",
            scroll="2m",
            body={"query": {"match_all": {}}, "_source": ["job_id"]},
            size=10000
        )

        while scroll["hits"]["hits"]:
            for hit in scroll["hits"]["hits"]:
                existing_ids.add(hit["_source"]["job_id"])
            scroll = elasticsearch_client.scroll(
                scroll_id=scroll["_scroll_id"],
                scroll="2m"
            )

        # Step 2: Fetch only non-indexed jobs from PostgreSQL
        if existing_ids:
            query = text(f'''
                    SELECT j.id AS job_id, j.description AS description
                    FROM {internships_table} j
                    WHERE j.id NOT IN :existing_ids
                ''')
            results = session.execute(query, {"existing_ids": tuple(existing_ids)}).fetchall()
        else:
            # If no existing IDs, just grab all jobs
            query = text(f'''
                    SELECT j.id AS job_id, j.description AS description
                    FROM {internships_table} j
                ''')
            results = session.execute(query).fetchall()

        # Get the id and description from teh jobs, since this is all we need
        jobs = [{'id': row.job_id, 'description': row.description} for row in results]

        # Step 3: Process in batches and upload to Elasticsearch
        total_uploaded = 0
        while jobs:
            jobs_batch = jobs[:50]
            jobs = jobs[50:]

            # Get embeddings
            embedded_jobs = get_description_embeddings(jobs_batch)

            # Prepare Elasticsearch bulk index actions
            actions = [
                {
                    "_index": "description-embeddings-internships",
                    "_source": {
                        "job_id": job["id"],
                        "description_embedding": job["embedding"]
                    }
                }
                for job in embedded_jobs if job["embedding"] is not None
            ]

            # Bulk upload to Elasticsearch
            if actions:
                helpers.bulk(elasticsearch_client, actions)
                total_uploaded += len(actions)

    except Exception as e:
        capture_exception(e)


def load_description_embeddings_internships_bulk_from_jobs(jobs):
    """
        This function loads the description embeddings into the index for any job from the jobs given, but not yet
        in the description-embeddings index. It first gets which jobs it needs to load, generates the embeddings, then
        loads them into the index

        :parameter jobs: A list of jobs to be used to load embeddings with
        :return: None
    """

    try:

        # Get the id and descriptions for all the jobs from the given jobs
        jobs = [{'id': job['id'], 'description': job['description']} for job in jobs]
        job_ids = [job['id'] for job in jobs]

        # Step 2: Check which IDs already exist in Elasticsearch
        existing_ids = set()
        query = {
            "query": {
                "terms": {
                    "job_id": job_ids
                }
            },
            "_source": ["job_id"]
        }

        try:
            response = elasticsearch_client.search(index="description-embeddings-internships", body=query, size=len(job_ids))
            existing_ids = {hit["_source"]["job_id"] for hit in response["hits"]["hits"]}
        except Exception as e:
            capture_exception(e)

        # Step 3: Filter out jobs that already exist
        jobs = [job for job in jobs if job["id"] not in existing_ids]

        # Step 3: Process in batches and upload to Elasticsearch
        total_uploaded = 0
        while jobs:
            # Batches of 50
            jobs_batch = jobs[:50]
            jobs = jobs[50:]

            # Get embeddings
            embedded_jobs = get_description_embeddings(jobs_batch)

            # Prepare Elasticsearch bulk index actions
            actions = [
                {
                    "_index": "description-embeddings-internships",
                    "_source": {
                        "job_id": job["id"],
                        "description_embedding": job["embedding"]
                    }
                }
                for job in embedded_jobs if job["embedding"] is not None
            ]

            # Bulk upload to Elasticsearch
            if actions:
                helpers.bulk(elasticsearch_client, actions)
                total_uploaded += len(actions)

    except Exception as e:
        capture_exception(e)


def get_description_document_count_internships():
    """
    Retrieve the total count of documents in the Elasticsearch description-embeddings index.
    """
    try:
        total_docs = elasticsearch_client.count(index="description-embeddings-internships")["count"]
        return total_docs
    except Exception as e:
        capture_exception(e)
        return None


def delete_orphan_documents_internships():
    """
    Delete documents from both Elasticsearch indexes ('title-embeddings' and 'description-embeddings') for internships and entry level
    where the job_id is not present in the internships table.
    """
    session = Session
    try:
        # Get all valid job_ids from PostgreSQL
        query = text(f'SELECT id FROM {internships_table}')
        valid_ids = {row.id for row in session.execute(query).fetchall()}
        session.commit()

        indexes = ["title-embeddings-internships", "description-embeddings-internships"]  # Indexes to delete orphan documents from
        total_deleted = 0

        for index_name in indexes:

            scroll = elasticsearch_client.search(
                index=index_name,
                scroll="2m",
                body={"query": {"match_all": {}}},
                _source=["job_id"],
                size=10000
            )

            delete_actions = []
            while scroll["hits"]["hits"]:
                for hit in scroll["hits"]["hits"]:
                    job_id = hit["_source"]["job_id"]
                    if job_id not in valid_ids:  # Add the id to the delete actions if it is an orphan
                        delete_actions.append({
                            "_op_type": "delete",
                            "_index": index_name,
                            "_id": hit["_id"]  # Use the actual document ID
                        })

                scroll = elasticsearch_client.scroll(
                    scroll_id=scroll["_scroll_id"],
                    scroll="2m"
                )

            if delete_actions:
                # Execute the deletes in bulk
                success, _ = bulk(elasticsearch_client, delete_actions)
                total_deleted += success

    except Exception as e:
        session.rollback()
        capture_exception(e)
    finally:
        session.remove()

def delete_duplicate_documents():
    """
    Deletes duplicate documents in both 'title-embeddings' and 'description-embeddings' indexes for internships and entry level
    based on job_id (keeps one document per job_id).
    """
    indexes = ["title-embeddings-internships", "description-embeddings-internships"]

    for index_name in indexes:
        job_id_map = {}  # job_id -> list of document IDs

        # Step 1: Scroll through all documents to find duplicates
        try:
            scroll = elasticsearch_client.search(
                index=index_name,
                scroll="2m",
                body={"query": {"match_all": {}}},
                _source=["job_id"],
                size=1000
            )

            while scroll["hits"]["hits"]:
                for hit in scroll["hits"]["hits"]:
                    doc_id = hit["_id"]
                    job_id = hit["_source"].get("job_id")
                    if job_id is not None:
                        job_id_map.setdefault(job_id, []).append(doc_id)

                scroll = elasticsearch_client.scroll(
                    scroll_id=scroll["_scroll_id"],
                    scroll="2m"
                )
        except Exception as e:
            capture_exception(e)
            continue

        # Step 2: Prepare delete actions for duplicates
        delete_actions = []
        for job_id, doc_ids in job_id_map.items():
            if len(doc_ids) > 1:
                # Keep the first one, delete the rest
                for duplicate_id in doc_ids[1:]:
                    delete_actions.append({
                        "_op_type": "delete",
                        "_index": index_name,
                        "_id": duplicate_id
                    })

        # Step 3: Bulk delete duplicates
        if delete_actions:
            try:
                success, _ = bulk(elasticsearch_client, delete_actions)
            except Exception as e:
                capture_exception(e)


def copy_index_documents(old_index: str, new_index: str):
    """
    Copy all documents from an existing Elasticsearch index into another existing index
    using the _reindex API.

    :param old_index: Name of the source index
    :param new_index: Name of the destination index (must already exist)
    :return: Response dict from Elasticsearch
    """
    try:
        resp = elasticsearch_client.reindex(
            source={"index": old_index},
            dest={"index": new_index},
            # run as a single blocking request, but give ES more time
            wait_for_completion=True,
            request_timeout=600,   # seconds
            refresh=True,
        )
        print(resp)
        return resp

    except Exception as e:
        capture_exception(e)
        print(f"Error copying documents from {old_index} to {new_index}: {e}")
        return None



# Call the function
if __name__ == "__main__":
    print("ELASTICSEARCH_CONFIG.PY file")